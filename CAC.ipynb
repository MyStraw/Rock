{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e0cb0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\dacon\\Rock\\rock\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm.auto import tqdm\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fb1de8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce GTX 1660 SUPER\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name()) if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a60f53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_SIZE': 299,\n",
    "    'EPOCHS': 1,\n",
    "    'LEARNING_RATE': 3e-4,\n",
    "    'BATCH_SIZE': 16,\n",
    "    'SEED': 777\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef936e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdf6e462",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PadSquare(A.ImageOnlyTransform):\n",
    "    def __init__(self, value=0, always_apply=False, p=1.0):\n",
    "        super().__init__(always_apply, p)\n",
    "        self.value = value\n",
    "\n",
    "    def apply(self, image, **params):\n",
    "        h, w, c = image.shape\n",
    "        max_dim = max(h, w)\n",
    "        pad_h = max_dim - h\n",
    "        pad_w = max_dim - w\n",
    "        top = pad_h // 2\n",
    "        bottom = pad_h - top\n",
    "        left = pad_w // 2\n",
    "        right = pad_w - left\n",
    "        return cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=self.value)\n",
    "\n",
    "    def get_transform_init_args_names(self):\n",
    "        return (\"value\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dde927e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_path_list, label_list, transforms=None):\n",
    "        self.img_path_list = img_path_list\n",
    "        self.label_list = label_list\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.img_path_list[index]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image=image)['image']\n",
    "        if self.label_list is not None:\n",
    "            return image, self.label_list[index]\n",
    "        else:\n",
    "            return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "484c67f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_transform = A.Compose([\n",
    "    PadSquare(value=(0, 0, 0)),\n",
    "    A.Resize(CFG['IMG_SIZE'], CFG['IMG_SIZE']),\n",
    "    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "test_transform = train_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afaa438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=128):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, 1, 1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(), nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.fc = nn.Linear(128, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac176826",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, latent_dim=128, num_classes=7):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(latent_dim, num_classes)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.fc(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25a67f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoClassifierUnit(nn.Module):\n",
    "    def __init__(self, latent_dim=128, num_classes=7):\n",
    "        super().__init__()\n",
    "        self.generator = Generator(latent_dim)\n",
    "        self.classifier = Classifier(latent_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.generator(x)\n",
    "        return z, self.classifier(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a958ea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CooperativeAutoClassifier(nn.Module):\n",
    "    def __init__(self, latent_dim=128, num_classes=7):\n",
    "        super().__init__()\n",
    "        self.acus = nn.ModuleList([AutoClassifierUnit(latent_dim, num_classes) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        zs, logits_list = [], []\n",
    "        for acu in self.acus:\n",
    "            z, logits = acu(x)\n",
    "            zs.append(z)\n",
    "            logits_list.append(logits)\n",
    "        return zs, logits_list\n",
    "\n",
    "    def compute_loss(self, logits_list, zs, labels, lambda_coop=0.1):\n",
    "        ce_loss = sum(F.cross_entropy(logits, labels) for logits in logits_list) / len(logits_list)\n",
    "        coop_loss = sum(F.mse_loss(zs[i], zs[j]) for i in range(len(zs)) for j in range(i + 1, len(zs)))\n",
    "        coop_loss = 2 * coop_loss / (len(zs) * (len(zs) - 1))\n",
    "        return ce_loss + lambda_coop * coop_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4765dcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, train_loader, val_loader, optimizer, device, epochs):\n",
    "    model.to(device)\n",
    "    best_f1, best_model = 0, None\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            zs, logits_list = model(imgs)\n",
    "            loss = model.compute_loss(logits_list, zs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        val_f1 = validate(model, val_loader, device)\n",
    "        print(f\"Epoch {epoch}: Train Loss={np.mean(train_losses):.4f}, Val F1={val_f1:.4f}\")\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_model = model.state_dict()\n",
    "            \n",
    "            save_path = \"best_model(cac).pth\"\n",
    "            torch.save(best_model, save_path)\n",
    "            print(f\"Best model saved (epoch {epoch}, F1={val_f1:.4f}) → {save_path}\")\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad16c487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            zs, logits_list = model(imgs)\n",
    "            avg_logits = sum(logits_list) / len(logits_list)\n",
    "            preds.extend(avg_logits.argmax(1).cpu().numpy())\n",
    "            targets.extend(labels.numpy())\n",
    "    return f1_score(targets, preds, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad77901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, test_loader, device, label_encoder):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for imgs in tqdm(test_loader):\n",
    "            imgs = imgs.to(device)\n",
    "            _, logits_list = model(imgs)\n",
    "            avg_logits = sum(logits_list) / len(logits_list)\n",
    "            pred_labels = avg_logits.argmax(1).cpu().numpy().tolist()\n",
    "            preds.extend(pred_labels)\n",
    "    return label_encoder.inverse_transform(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1d9ad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "df = pd.DataFrame({'img_path': glob.glob('./train/*/*')})\n",
    "df['rock_type'] = df['img_path'].apply(lambda x: Path(x).parts[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cb6d90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      img_path       rock_type\n",
      "0             ./train\\Andesite\\TRAIN_00000.jpg        Andesite\n",
      "1             ./train\\Andesite\\TRAIN_00001.jpg        Andesite\n",
      "2             ./train\\Andesite\\TRAIN_00002.jpg        Andesite\n",
      "3             ./train\\Andesite\\TRAIN_00003.jpg        Andesite\n",
      "4             ./train\\Andesite\\TRAIN_00004.jpg        Andesite\n",
      "...                                        ...             ...\n",
      "380015  ./train\\Weathered_Rock\\TRAIN_37164.jpg  Weathered_Rock\n",
      "380016  ./train\\Weathered_Rock\\TRAIN_37165.jpg  Weathered_Rock\n",
      "380017  ./train\\Weathered_Rock\\TRAIN_37166.jpg  Weathered_Rock\n",
      "380018  ./train\\Weathered_Rock\\TRAIN_37167.jpg  Weathered_Rock\n",
      "380019  ./train\\Weathered_Rock\\TRAIN_37168.jpg  Weathered_Rock\n",
      "\n",
      "[380020 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88f4bfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['rock_type'], random_state=CFG['SEED'])\n",
    "le = preprocessing.LabelEncoder()\n",
    "train_df['rock_type'] = le.fit_transform(train_df['rock_type'])\n",
    "val_df['rock_type'] = le.transform(val_df['rock_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7166fbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_df['img_path'].values, train_df['rock_type'].values, train_transform)\n",
    "val_dataset = CustomDataset(val_df['img_path'].values, val_df['rock_type'].values, test_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ab29f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 19001/19001 [1:39:05<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=1.1385, Val F1=0.5223\n",
      "Best model saved (epoch 1, F1=0.5223) → best_model(cac).pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CooperativeAutoClassifier(latent_dim=128, num_classes=7)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=CFG['LEARNING_RATE'])\n",
    "best_weights = train_fn(model, train_loader, val_loader, optimizer, device, CFG['EPOCHS'])\n",
    "model.load_state_dict(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99b9b6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 140/5938 [00:30<21:02,  4.59it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m CustomDataset(test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_path\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues, \u001b[38;5;28;01mNone\u001b[39;00m, test_transform)\n\u001b[0;32m      3\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39mCFG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBATCH_SIZE\u001b[39m\u001b[38;5;124m'\u001b[39m], shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mle\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 5\u001b[0m, in \u001b[0;36minference\u001b[1;34m(model, test_loader, device, label_encoder)\u001b[0m\n\u001b[0;32m      3\u001b[0m preds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m imgs \u001b[38;5;129;01min\u001b[39;00m tqdm(test_loader):\n\u001b[0;32m      6\u001b[0m         imgs \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m         _, logits_list \u001b[38;5;241m=\u001b[39m model(imgs)\n",
      "File \u001b[1;32mb:\\dacon\\Rock\\rock\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mb:\\dacon\\Rock\\rock\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    739\u001b[0m ):\n",
      "File \u001b[1;32mb:\\dacon\\Rock\\rock\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mb:\\dacon\\Rock\\rock\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mb:\\dacon\\Rock\\rock\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m      8\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_path_list[index]\n\u001b[1;32m----> 9\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('./test.csv')\n",
    "test_dataset = CustomDataset(test_df['img_path'].values, None, test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)\n",
    "preds = inference(model, test_loader, device, le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf04cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit['rock_type'] = preds\n",
    "submit.to_csv('./baseline_submit(cac).csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
